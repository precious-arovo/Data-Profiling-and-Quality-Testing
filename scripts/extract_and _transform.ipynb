{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and Transform Notebook\n",
    "\n",
    "Description:\n",
    "This notebook is designed to download the [Stock Market Dataset](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset/data) - historical daily prices of Nasdaq-traded stocks and ETFs from Kaggle, and transform the data to a useable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary dependencies.\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from kaggle\n",
    "# !kaggle datasets download -d jacksoncrow/stock-market-dataset -p \"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.12 / client 1.6.11)\n",
      "Downloaded metadata to C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\\dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Download the metadata\n",
    "# !kaggle datasets metadata jacksoncrow/stock-market-dataset -p \"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the downloaded zip file and the folder to extarct the files to.\n",
    "zipped_file = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\\stock-market-dataset.zip\"\n",
    "extracted = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted' already exists.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\USER\\\\Desktop\\\\Projects\\\\Data-Profiling-and-Quality-Testing\\\\data\\\\extracted\\\\etfs\\\\PRN.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Extract files to the destination directory\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zipped_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mzip_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextracted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files extracted to: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\predict\\lib\\zipfile.py:1642\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[1;32m-> 1642\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\predict\\lib\\zipfile.py:1696\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         os\u001b[38;5;241m.\u001b[39mmkdir(targetpath)\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m-> 1696\u001b[0m      \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[0;32m   1697\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(source, target)\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\USER\\\\Desktop\\\\Projects\\\\Data-Profiling-and-Quality-Testing\\\\data\\\\extracted\\\\etfs\\\\PRN.csv'"
     ]
    }
   ],
   "source": [
    "# Extracting the zip file\n",
    "if not os.path.exists(extracted):\n",
    "    os.makedirs(extracted)\n",
    "    print(f\"Directory created: '{extracted}'\")\n",
    "else:\n",
    "    print(f\"Directory '{extracted}' already exists.\")\n",
    "\n",
    "# Extract files to the destination directory\n",
    "with zipfile.ZipFile(zipped_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path=extracted)\n",
    "\n",
    "print(f\"All files extracted to: '{extracted}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further manual operations on the zip file shows that the error above is due to fact that the PRN.csv file is named after some windows reserved names for specific operations. In this case, PRN - print, for print operation.\n",
    "Thus, to extract the files in the zip file, the PRN.csv file would be renamed to PRN_FILE.csv before extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files extracted to: C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zipped_file, 'r') as zip_ref:\n",
    "    for file in zip_ref.infolist():\n",
    "        original_filename = file.filename\n",
    "        # Ensure filename replacements are correct and assigned properly\n",
    "        filename = original_filename.replace('PRN.csv', 'PRN_FILE.csv')\n",
    "        \n",
    "        # Define the full path for the extracted file\n",
    "        path = os.path.join(extracted, filename)\n",
    "\n",
    "        # Ensure the directory exists where the file will be extracted.\n",
    "        directory = os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory, exist_ok=True)  # Use exist_ok=True\n",
    "    \n",
    "        # Extracting to path. Overwrite if files already exist in path.\n",
    "        with zip_ref.open(file) as source, open(path, 'wb') as target:\n",
    "            shutil.copyfileobj(source, target)\n",
    "\n",
    "print(f\"All files extracted to: {extracted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the stock files into one\n",
    "def merge_stock_files(input_path, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple stock CSV files in a director into a single CSV file.\n",
    "    A new column 'Stock' will be added to the single CSV, containig the stock name derived from each file name.\n",
    "    \n",
    "    Arg:\n",
    "    input_path: The path to the directory containing all the CSV files to be merged.\n",
    "    output_path: The path to where the merged CSV file wil be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty list for the stocks names.\n",
    "    stocks = []\n",
    "\n",
    "    # Iterate over each file in the input path.\n",
    "    for filename in os.listdir(input_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Define the full path to the file\n",
    "            file_path = os.path.join(input_path, filename)\n",
    "\n",
    "            # Read the CSV file paths into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract the stock name from the filename\n",
    "            stock_name = filename.replace('.csv', '')\n",
    "\n",
    "            # Add a new column with the stock name\n",
    "            df['Stcoks'] = stock_name\n",
    "\n",
    "            # Append the dataframe to the list\n",
    "            stocks.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    combined_df = pd.concat(stocks)\n",
    "\n",
    "    # Save the combined dataframe to a CSV file\n",
    "    combined_df.to_csv(output_file, index=True)\n",
    "\n",
    "    print(f\"Combined CSV file creadted at '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file creadted at 'C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\etfs\\etfs.csv'\n"
     ]
    }
   ],
   "source": [
    "# Merge the etfs CSV\n",
    "input_path = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\\etfs\"\n",
    "output_file = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\etfs\\etfs.csv\"\n",
    "merge_stock_files(input_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file creadted at 'C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\stocks\\stocks.csv'\n"
     ]
    }
   ],
   "source": [
    "# Merge the stocks CSV\n",
    "input_path = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\\stocks\"\n",
    "output_file = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\stocks\\stocks.csv\"\n",
    "merge_stock_files(input_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict",
   "language": "python",
   "name": "predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

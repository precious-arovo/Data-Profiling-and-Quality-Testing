{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extract and Transformation\n",
    "\n",
    "Script to download and transform Stock Market Dataset - historical daily prices of Nasdaq-traded stocks and ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary dependencies.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import shutil\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Row\n",
    "from pyspark.sql.types import StructField, StructType, FloatType, StringType, DateType\n",
    "from pyspark.sql.functions import min, max, year, month, dayofmonth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from kaggle\n",
    "# !kaggle datasets download -d jacksoncrow/stock-market-dataset -p \"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the metadata\n",
    "# !kaggle datasets metadata jacksoncrow/stock-market-dataset -p \"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the downloaded zip file and the folder to extarct the files to.\n",
    "zipped_file = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\raw_data\\stock-market-dataset.zip\"\n",
    "extracted = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted' already exists.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\USER\\\\Desktop\\\\Projects\\\\Data-Profiling-and-Quality-Testing\\\\data\\\\extracted\\\\etfs\\\\PRN.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Extract files to the destination directory\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zipped_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mzip_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextracted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files extracted to: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\predict\\lib\\zipfile.py:1642\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[1;32m-> 1642\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\predict\\lib\\zipfile.py:1696\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         os\u001b[38;5;241m.\u001b[39mmkdir(targetpath)\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m-> 1696\u001b[0m      \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[0;32m   1697\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(source, target)\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\USER\\\\Desktop\\\\Projects\\\\Data-Profiling-and-Quality-Testing\\\\data\\\\extracted\\\\etfs\\\\PRN.csv'"
     ]
    }
   ],
   "source": [
    "# Extracting the zip file\n",
    "if not os.path.exists(extracted):\n",
    "    os.makedirs(extracted)\n",
    "    print(f\"Directory created: '{extracted}'\")\n",
    "else:\n",
    "    print(f\"Directory '{extracted}' already exists.\")\n",
    "\n",
    "# Extract files to the destination directory\n",
    "with zipfile.ZipFile(zipped_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path=extracted)\n",
    "\n",
    "print(f\"All files extracted to: '{extracted}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further manual operations on the zip file shows that the error above is due to fact that the PRN.csv file is named after some windows reserved names for specific operations. In this case, PRN - print, for print operation.\n",
    "Thus, to extract the files in the zip file, the PRN.csv file would be renamed to PRN_FILE.csv before extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files extracted to: C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zipped_file, 'r') as zip_ref:\n",
    "    for file in zip_ref.infolist():\n",
    "        original_filename = file.filename\n",
    "        # Ensure filename replacements are correct and assigned properly\n",
    "        filename = original_filename.replace('PRN.csv', 'PRN_FILE.csv')\n",
    "        \n",
    "        # Define the full path for the extracted file\n",
    "        path = os.path.join(extracted, filename)\n",
    "\n",
    "        # Ensure the directory exists where the file will be extracted.\n",
    "        directory = os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory, exist_ok=True)  # Use exist_ok=True\n",
    "    \n",
    "        # Extracting to path. Overwrite if files already exist in path.\n",
    "        with zip_ref.open(file) as source, open(path, 'wb') as target:\n",
    "            shutil.copyfileobj(source, target)\n",
    "\n",
    "print(f\"All files extracted to: {extracted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the stock files into one\n",
    "def merge_stock_files(input_path, output_file):\n",
    "    \"\"\"\n",
    "    Merge multiple stock CSV files in a director into a single CSV file.\n",
    "    A new column 'Stock' will be added to the single CSV, containig the stock name derived from each file name.\n",
    "    \n",
    "    Arg:\n",
    "    input_path: The path to the directory containing all the CSV files to be merged.\n",
    "    output_path: The path to where the merged CSV file wil be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty list for the stocks names.\n",
    "    stocks = []\n",
    "\n",
    "    # Iterate over each file in the input path.\n",
    "    for filename in os.listdir(input_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Define the full path to the file\n",
    "            file_path = os.path.join(input_path, filename)\n",
    "\n",
    "            # Read the CSV file paths into a dataframe\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract the stock name from the filename\n",
    "            stock_name = filename.replace('.csv', '')\n",
    "\n",
    "            # Add a new column with the stock name\n",
    "            df['Stcoks'] = stock_name\n",
    "\n",
    "            # Append the dataframe to the list\n",
    "            stocks.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    combined_df = pd.concat(stocks)\n",
    "\n",
    "    # Save the combined dataframe to a CSV file\n",
    "    combined_df.to_csv(output_file, index=True)\n",
    "\n",
    "    print(f\"Combined CSV file created at '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created at 'C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\etfs\\etfs.csv'\n"
     ]
    }
   ],
   "source": [
    "# Merge the etfs CSV\n",
    "input_path = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\\etfs\"\n",
    "output_file = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\etfs\\etfs.csv\"\n",
    "merge_stock_files(input_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file created at 'C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\stocks\\stocks.csv'\n"
     ]
    }
   ],
   "source": [
    "# Merge the stocks CSV\n",
    "input_path = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\extracted\\stocks\"\n",
    "output_file = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\stocks\\stocks.csv\"\n",
    "merge_stock_files(input_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing exploratory analysis on the created files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment vairables for PySpark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SparkContext and SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "etfs = spark.read.csv(r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\etfs\\etfs.csv\", header=True)\n",
    "stocks = spark.read.csv(r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\merged\\stocks\\stocks.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The etfs dataset has (3954316, 9) dimension.\n",
      "The stocks dataset has (24197442, 9) dimension.\n"
     ]
    }
   ],
   "source": [
    "# Dimension of the datasets\n",
    "\n",
    "print(f\"The etfs dataset has ({etfs.count()}, {len(etfs.columns)}) dimension.\")\n",
    "print(f\"The stocks dataset has ({stocks.count()}, {len(stocks.columns)}) dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Stcoks: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dataset schema\n",
    "etfs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Stcoks: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+------------------+------------------+------------------+--------+------+\n",
      "|_c0|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|Stcoks|\n",
      "+---+----------+------------------+------------------+------------------+------------------+------------------+--------+------+\n",
      "|  0|2018-08-15| 11.84000015258789| 11.84000015258789|11.739999771118164|11.739999771118164|11.739999771118164| 27300.0|  AAAU|\n",
      "|  1|2018-08-16|11.779999732971191|11.800000190734863|11.739999771118164|11.739999771118164|11.739999771118164|428400.0|  AAAU|\n",
      "|  2|2018-08-17|11.800000190734863| 11.81999969482422|11.770000457763672| 11.81999969482422| 11.81999969482422| 52400.0|  AAAU|\n",
      "|  3|2018-08-20|11.880000114440918| 11.90999984741211|11.850000381469728|11.899999618530272|11.899999618530272| 28700.0|  AAAU|\n",
      "|  4|2018-08-21|11.920000076293944|11.949999809265137|11.890000343322754| 11.93000030517578| 11.93000030517578| 30600.0|  AAAU|\n",
      "+---+----------+------------------+------------------+------------------+------------------+------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etfs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+------------------+------------------+------------------+----------+------+\n",
      "|_c0|      Date|              Open|              High|               Low|             Close|         Adj Close|    Volume|Stcoks|\n",
      "+---+----------+------------------+------------------+------------------+------------------+------------------+----------+------+\n",
      "|  0|1999-11-18| 32.54649353027344|   35.765380859375|28.612302780151367|31.473533630371094| 27.06866455078125|62546300.0|     A|\n",
      "|  1|1999-11-19|30.713520050048828| 30.75822639465332| 28.47818374633789|28.880542755126957| 24.83857727050781|15234100.0|     A|\n",
      "|  2|1999-11-22|29.551143646240234|31.473533630371094| 28.65700912475586|31.473533630371094| 27.06866455078125| 6577800.0|     A|\n",
      "|  3|1999-11-23| 30.40057182312012|31.205293655395508|28.612302780151367|28.612302780151367|24.607879638671875| 5975600.0|     A|\n",
      "|  4|1999-11-24|28.701717376708984|29.998210906982425|28.612302780151367|29.372318267822266|25.261524200439453| 4843200.0|     A|\n",
      "+---+----------+------------------+------------------+------------------+------------------+------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the columns in both dataset.\n",
    "# Convert to lowercase and replace white space with underscore.\n",
    "\n",
    "for column in etfs.columns:\n",
    "    etfs = etfs.withColumnRenamed(column, '_'.join(column.split()).lower())\n",
    "\n",
    "for column in stocks.columns:\n",
    "    stocks = stocks.withColumnRenamed(column, '_'.join(column.split()).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stcoks']\n",
      "['_c0', 'date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stcoks']\n"
     ]
    }
   ],
   "source": [
    "print(etfs.columns)\n",
    "print(stocks.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the _c0 column in both datasets\n",
    "etfs = etfs.drop(\"_c0\")\n",
    "stocks = stocks.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The etfs dataset has (3954316, 8) dimension.\n",
      "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stcoks']\n",
      "The stocks dataset has (24197442, 8) dimension.\n",
      "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stcoks']\n"
     ]
    }
   ],
   "source": [
    "# Confirm the drop action by checking dimension.\n",
    "\n",
    "print(f\"The etfs dataset has ({etfs.count()}, {len(etfs.columns)}) dimension.\")\n",
    "print(etfs.columns)\n",
    "print(f\"The stocks dataset has ({stocks.count()}, {len(stocks.columns)}) dimension.\")\n",
    "print(stocks.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typecast to convert the stringtypes datatypes in the dataset to their appropriate datatypes.\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('open', FloatType(), True),\n",
    "    StructField('high', FloatType(), True),\n",
    "    StructField('low', FloatType(), True),\n",
    "    StructField('close', FloatType(), True),\n",
    "    StructField('adj_close', FloatType(), True),\n",
    "    StructField('volume', FloatType(), True),\n",
    "    StructField('stocks', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to the Dataframe\n",
    "\n",
    "etfs_data = etfs.select([etfs[column].cast(schema.fields[i].dataType) for i, column in enumerate(etfs.columns)])\n",
    "stocks_data = stocks.select([stocks[column].cast(schema.fields[i].dataType) for i, column in enumerate(stocks.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stcoks: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etfs_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adj_close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- stcoks: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date range for the etfs stocks data is: (1986-04-03, 2020-04-01).\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the date range for the etfs dataset.\n",
    "date_range = etfs_data.select(min(\"date\").alias(\"min_date\"), max(\"date\").alias(\"max_date\")).first()\n",
    "\n",
    "# Retrieve the minimum and maximum dates\n",
    "min_date = date_range[\"min_date\"]\n",
    "max_date = date_range[\"max_date\"]\n",
    "\n",
    "print(f\"The date range for the etfs stocks data is: ({min_date}, {max_date}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date range for the stocks data is: (1962-01-02, 2020-04-02).\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the date range for the stocks dataset.\n",
    "date_range = stocks_data.select(min(\"date\").alias(\"min_date\"), max(\"date\").alias(\"max_date\")).first()\n",
    "\n",
    "# Retrieve the minimum and maximum dates\n",
    "min_date = date_range[\"min_date\"]\n",
    "max_date = date_range[\"max_date\"]\n",
    "\n",
    "print(f\"The date range for the stocks data is: ({min_date}, {max_date}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_by_date(data, output_dir, file_name):\n",
    "    \"\"\"\n",
    "    Function to re-organize a stock historical dataset.\n",
    "    Organize by date and save in a directory such as \".../year/month/day/file.csv\"\n",
    "\n",
    "    Args:\n",
    "        data: the dataset to be organized.\n",
    "        output_dir: the output directory.\n",
    "        file_name: specifies the file name to be created.\n",
    "    \"\"\"\n",
    "    # Extract year, month, and day from the date column.\n",
    "    data = data.withColumn(\"year\", F.year('date'))\n",
    "    data = data.withColumn(\"month\", F.month('date'))\n",
    "    data = data.withColumn(\"day\", F.dayofmonth('date'))\n",
    "\n",
    "    # Repartition data to improve parallelism\n",
    "    data = data.repartition(\"year\", \"month\", \"day\")\n",
    "\n",
    "    # Group data by year, month, and day and aggregate to a single CSV file\n",
    "    grouped_df = data.groupBy(\"year\", \"month\", \"day\").agg(F.collect_list(F.struct(*data.columns)).alias(\"data\"))\n",
    "\n",
    "    # Iterate over each group and write to a single CSV file\n",
    "    for row in grouped_df.collect():\n",
    "        year_val = row[\"year\"]\n",
    "        month_val = row[\"month\"]\n",
    "        day_val = row[\"day\"]\n",
    "\n",
    "        # Create the directory structure\n",
    "        year_dir = os.path.join(output_dir, str(year_val))\n",
    "        month_dir = os.path.join(year_dir, str(month_val))\n",
    "        day_dir = os.path.join(month_dir, str(day_val))\n",
    "        os.makedirs(day_dir, exist_ok=True)\n",
    "\n",
    "        # Write data to CSV file\n",
    "        file_path = os.path.join(day_dir, file_name)\n",
    "        spark.createDataFrame(row[\"data\"]).coalesce(1).write.csv(file_path, header=True, mode=\"overwrite\")\n",
    "    \n",
    "    print(f\"'{data}' re-organized and saved to: '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = etfs_data  \n",
    "output_dir = ''\n",
    "file_name = \"etf.csv\"\n",
    "\n",
    "organize_by_date(data, output_dir, file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict",
   "language": "python",
   "name": "predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

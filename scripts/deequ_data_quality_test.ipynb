{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Test using Deequ\n",
    "\n",
    "Deequ is a data quality management tool developed by Amazon, designed to help ensure the quality of data in large-scale pipelines. Deequ can be used to define data quality constraints and run checks against data sources to ensure that they meet those constraints.\n",
    "\n",
    "PyDeequ is the Pythin Deequ, which allows users to interact with Deequ functionality directly from Python code, providing more convenience for Python users to utilize the capabilities of Deequ in daat processing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13680\\1506829307.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pydeequ\n",
    "import json\n",
    "import sagemaker_pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, Row, DataFrame, functions as F \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.profiles import *\n",
    "from pydeequ.suggestions import *\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5106"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest the stock data for the specific year.\n",
    "stock_data_path = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\transformed\\1962_stock_data\"\n",
    "stock_data = spark.read.parquet(stock_data_path)\n",
    "stock_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stock']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the display options to prevent truncating for whe using .toPandas to display result.\n",
    "pd.set_option('display.max_rows', None) # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)    # Disable column witdth restriction\n",
    "pd.set_option('display.max_colwidth', None)    # Disable column content width restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test for Completeness/Null Values**\n",
    "\n",
    "Testing for null values using **Verification Suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(date,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(open,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(high,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(low,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9917743830787309 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(close,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(adj_close,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9958871915393654 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(volume,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9958871915393654 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Completeness Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(stock,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     check check_level check_status  \\\n",
       "0  Data Completeness Check       Error        Error   \n",
       "1  Data Completeness Check       Error        Error   \n",
       "2  Data Completeness Check       Error        Error   \n",
       "3  Data Completeness Check       Error        Error   \n",
       "4  Data Completeness Check       Error        Error   \n",
       "5  Data Completeness Check       Error        Error   \n",
       "6  Data Completeness Check       Error        Error   \n",
       "7  Data Completeness Check       Error        Error   \n",
       "\n",
       "                                             constraint constraint_status  \\\n",
       "0       CompletenessConstraint(Completeness(date,None))           Success   \n",
       "1       CompletenessConstraint(Completeness(open,None))           Success   \n",
       "2       CompletenessConstraint(Completeness(high,None))           Success   \n",
       "3        CompletenessConstraint(Completeness(low,None))           Failure   \n",
       "4      CompletenessConstraint(Completeness(close,None))           Success   \n",
       "5  CompletenessConstraint(Completeness(adj_close,None))           Failure   \n",
       "6     CompletenessConstraint(Completeness(volume,None))           Failure   \n",
       "7      CompletenessConstraint(Completeness(stock,None))           Success   \n",
       "\n",
       "                                                    constraint_message  \n",
       "0                                                                       \n",
       "1                                                                       \n",
       "2                                                                       \n",
       "3  Value: 0.9917743830787309 does not meet the constraint requirement!  \n",
       "4                                                                       \n",
       "5  Value: 0.9958871915393654 does not meet the constraint requirement!  \n",
       "6  Value: 0.9958871915393654 does not meet the constraint requirement!  \n",
       "7                                                                       "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completeness test\n",
    "\n",
    "# Set up PyDeequ for completeness check\n",
    "check = Check(spark, CheckLevel.Error, \"Data Completeness Check\")\n",
    "\n",
    "# Loop through the columns of the stock data\n",
    "for column in df.columns:\n",
    "    checkResult = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check.isComplete(column)\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "resultDataFrame = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "resultDataFrame.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test for Zeros**\n",
    "\n",
    "Checking for the presence of entries with **zero** within the dataset using the **Verification Suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying only the numerical columns\n",
    "numerical_cols = ['open', 'high', 'low', 'close', 'adj_close', 'volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(open,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(high,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(low,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.05237788334488869 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(close,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.05362497642636299 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(adj_close,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 4.0381453914051235E-7 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zero Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(volume,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               check check_level check_status  \\\n",
       "0  Zero Values Check       Error        Error   \n",
       "1  Zero Values Check       Error        Error   \n",
       "2  Zero Values Check       Error        Error   \n",
       "3  Zero Values Check       Error        Error   \n",
       "4  Zero Values Check       Error        Error   \n",
       "5  Zero Values Check       Error        Error   \n",
       "\n",
       "                                   constraint constraint_status  \\\n",
       "0       MinimumConstraint(Minimum(open,None))           Success   \n",
       "1       MinimumConstraint(Minimum(high,None))           Success   \n",
       "2        MinimumConstraint(Minimum(low,None))           Failure   \n",
       "3      MinimumConstraint(Minimum(close,None))           Failure   \n",
       "4  MinimumConstraint(Minimum(adj_close,None))           Failure   \n",
       "5     MinimumConstraint(Minimum(volume,None))           Success   \n",
       "\n",
       "                                                       constraint_message  \n",
       "0                                                                          \n",
       "1                                                                          \n",
       "2    Value: 0.05237788334488869 does not meet the constraint requirement!  \n",
       "3    Value: 0.05362497642636299 does not meet the constraint requirement!  \n",
       "4  Value: 4.0381453914051235E-7 does not meet the constraint requirement!  \n",
       "5                                                                          "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up PyDeequ for Zero Values Check\n",
    "check_zero = Check(spark, CheckLevel.Error, \"Zero Values Check\")\n",
    "\n",
    "# Looping through the numerical columns of the dataset\n",
    "for column in numerical_cols:\n",
    "    checkResult_zero = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            check_zero.hasMin(column, lambda x: x == 0)\n",
    "        ) \\\n",
    "        .run()\n",
    "\n",
    "# Displaying the results\n",
    "resultDataFrame_zero = VerificationResult.checkResultsAsDataFrame(spark, checkResult_zero)\n",
    "resultDataFrame_zero.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test fo Negative Values**\n",
    "\n",
    "Check for **Negative Values** in the dataset using **Verification Suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(open is non-negative,COALESCE(CAST(open AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(high is non-negative,COALESCE(CAST(high AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(low is non-negative,COALESCE(CAST(low AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(close is non-negative,COALESCE(CAST(close AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(adj_close is non-negative,COALESCE(CAST(adj_close AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Negative Values Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(volume is non-negative,COALESCE(CAST(volume AS DECIMAL(20,10)), 0.0) &gt;= 0,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   check check_level check_status  \\\n",
       "0  Negative Values Check       Error      Success   \n",
       "1  Negative Values Check       Error      Success   \n",
       "2  Negative Values Check       Error      Success   \n",
       "3  Negative Values Check       Error      Success   \n",
       "4  Negative Values Check       Error      Success   \n",
       "5  Negative Values Check       Error      Success   \n",
       "\n",
       "                                                                                                               constraint  \\\n",
       "0            ComplianceConstraint(Compliance(open is non-negative,COALESCE(CAST(open AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "1            ComplianceConstraint(Compliance(high is non-negative,COALESCE(CAST(high AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "2              ComplianceConstraint(Compliance(low is non-negative,COALESCE(CAST(low AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "3          ComplianceConstraint(Compliance(close is non-negative,COALESCE(CAST(close AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "4  ComplianceConstraint(Compliance(adj_close is non-negative,COALESCE(CAST(adj_close AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "5        ComplianceConstraint(Compliance(volume is non-negative,COALESCE(CAST(volume AS DECIMAL(20,10)), 0.0) >= 0,None))   \n",
       "\n",
       "  constraint_status constraint_message  \n",
       "0           Success                     \n",
       "1           Success                     \n",
       "2           Success                     \n",
       "3           Success                     \n",
       "4           Success                     \n",
       "5           Success                     "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up PyDeequ for Negative Values Check\n",
    "constraints = Check(spark, CheckLevel.Error, \"Negative Values Check\")\n",
    "\n",
    "# Looping through the numerical columns of the dataset\n",
    "for column in numerical_cols:\n",
    "    checkResult_zero = VerificationSuite(spark) \\\n",
    "        .onData(df) \\\n",
    "        .addCheck(\n",
    "            constraints.isNonNegative(column)\n",
    "        ) \\\n",
    "        .run()\n",
    "\n",
    "# Displaying the results\n",
    "resultDataFrame_zero = VerificationResult.checkResultsAsDataFrame(spark, checkResult_zero)\n",
    "resultDataFrame_zero.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test for Stock Tickers Consistency**\n",
    "\n",
    "Using **Verification Suite** to test for consistency in the stock tickers, in comparism with the information in the dataset metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nasdaq Traded</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security Name</th>\n",
       "      <th>Listing Exchange</th>\n",
       "      <th>Market Category</th>\n",
       "      <th>ETF</th>\n",
       "      <th>Round Lot Size</th>\n",
       "      <th>Test Issue</th>\n",
       "      <th>Financial Status</th>\n",
       "      <th>CQS Symbol</th>\n",
       "      <th>NASDAQ Symbol</th>\n",
       "      <th>NextShares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa Corporation Common Stock</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>AAAU</td>\n",
       "      <td>Perth Mint Physical Gold ETF</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AAAU</td>\n",
       "      <td>AAAU</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>AACG</td>\n",
       "      <td>ATA Creativity Global - American Depositary Shares, each representing two common shares</td>\n",
       "      <td>Q</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AACG</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>AADR</td>\n",
       "      <td>AdvisorShares Dorsey Wright ADR ETF</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td>Y</td>\n",
       "      <td>100.0</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AADR</td>\n",
       "      <td>AADR</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nasdaq Traded Symbol  \\\n",
       "0             Y      A   \n",
       "1             Y     AA   \n",
       "2             Y   AAAU   \n",
       "3             Y   AACG   \n",
       "4             Y   AADR   \n",
       "\n",
       "                                                                             Security Name  \\\n",
       "0                                                  Agilent Technologies, Inc. Common Stock   \n",
       "1                                                          Alcoa Corporation Common Stock    \n",
       "2                                                             Perth Mint Physical Gold ETF   \n",
       "3  ATA Creativity Global - American Depositary Shares, each representing two common shares   \n",
       "4                                                      AdvisorShares Dorsey Wright ADR ETF   \n",
       "\n",
       "  Listing Exchange Market Category ETF  Round Lot Size Test Issue  \\\n",
       "0                N                   N           100.0          N   \n",
       "1                N                   N           100.0          N   \n",
       "2                P                   Y           100.0          N   \n",
       "3                Q               G   N           100.0          N   \n",
       "4                P                   Y           100.0          N   \n",
       "\n",
       "  Financial Status CQS Symbol NASDAQ Symbol NextShares  \n",
       "0              NaN          A             A          N  \n",
       "1              NaN         AA            AA          N  \n",
       "2              NaN       AAAU          AAAU          N  \n",
       "3                N        NaN          AACG          N  \n",
       "4              NaN       AADR          AADR          N  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the metadata \"symbols_valid_meta.csv\" into pandas data frame.\n",
    "metadata_path = r\"C:\\Users\\USER\\Desktop\\Projects\\Data-Profiling-and-Quality-Testing\\data\\symbols_valid_meta.csv\"\n",
    "\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the metadata to DataFrame\n",
    "meta = spark.createDataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARNCA\n"
     ]
    }
   ],
   "source": [
    "distinct_stock = df.groupBy('stock').count()\n",
    "distinct_symbol = metadata['Symbol'].unique()\n",
    "\n",
    "# Converting both data to list\n",
    "stock_column = distinct_stock.toPandas()\n",
    "stock_column = stock_column.values.tolist()\n",
    "symbol_column = distinct_symbol.tolist()\n",
    "\n",
    "stock_symbol = [item[0] for item in stock_column]\n",
    "\n",
    "for stock in stock_symbol:\n",
    "    if stock not in symbol_column:\n",
    "        print(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stock Ticker Verification</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9958871915393654 does not meet the constraint requirement! The stock ticker is not listed in the metadata.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       check check_level check_status constraint_status  \\\n",
       "0  Stock Ticker Verification       Error        Error           Failure   \n",
       "\n",
       "                                                                                                    constraint_message  \n",
       "0  Value: 0.9958871915393654 does not meet the constraint requirement! The stock ticker is not listed in the metadata.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the meta DataFrame's 'Symbol' column to a Python list of allowed values\n",
    "allowed_stock_symbols = meta.select(\"Symbol\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Useing VerificationSuite from PyDeequ to verify the stock tickers in the DataFrame `df`\n",
    "verificationResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        Check(spark, CheckLevel.Error, \"Stock Ticker Verification\") \\\n",
    "            .isContainedIn(\"stock\", allowed_stock_symbols, \n",
    "                           hint=\"The stock ticker is not listed in the metadata.\")\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "# Displaying the results of the verification\n",
    "resultDataFrame_zero = VerificationResult.checkResultsAsDataFrame(spark, verificationResult)\n",
    "resultDataFrame_zero.toPandas().drop(columns=['constraint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test for Duplicates**\n",
    "\n",
    "Using **Verification Suite** to check for the uniqueness of the entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplication check results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duplication Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(Stream(stock, ?),None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9776733254994124 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               check check_level check_status  \\\n",
       "0  Duplication Check     Warning      Warning   \n",
       "\n",
       "                                                constraint constraint_status  \\\n",
       "0  UniquenessConstraint(Uniqueness(Stream(stock, ?),None))           Failure   \n",
       "\n",
       "                                                    constraint_message  \n",
       "0  Value: 0.9776733254994124 does not meet the constraint requirement!  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the primary key for uniqueness verification\n",
    "primary_key = ['stock', 'date']\n",
    "\n",
    "# Initializing VerificationSuite with the DataFrame\n",
    "verification_suite = VerificationSuite(spark).onData(df)\n",
    "\n",
    "# Defining a check for uniqueness\n",
    "duplication_check = Check(spark, CheckLevel.Warning, \"Duplication Check\")\\\n",
    "    .hasUniqueness(primary_key, lambda x: x == 1) # Corrected variable name\n",
    "\n",
    "# Adding the check to the VerificationSuite\n",
    "verification_result = verification_suite.addCheck(duplication_check).run()\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Duplication check results:\")\n",
    "results_df = VerificationResult.checkResultsAsDataFrame(spark, verification_result)\n",
    "results_df.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict",
   "language": "python",
   "name": "predict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
